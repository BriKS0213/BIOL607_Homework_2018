---
title: "T-Test and ChiSq Homework"
author: "Brianna Shaughnessy"
date: "10/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(readr)
library(tidyverse)
library(broom)
library(beyonce)
```

#1. W&S ChiSq Questions

##Chapter 8
####12a. Calculate the fraction of b alleles in the population (remember, each bear has two copies of the gene)

Out of 87 bears sampled in this population there are 87 X 2 possible alleles (174) alleles. Looking at my frequency table there are 21 bears with the bb genotype, 24 with Bb, and 42 with BB. This sums to 66 (b) alleles in this sample population. **The fraction of b alleles in this population is 66/174 or ~38%.

####12b. With your estimate of the fraction of b alleles, and assuming a binomial distribution, calculate the expected frequency of bears with 0, 1, and 2 copies.

Expected frequency of 0: 33.5  
Expected frequency of 1: 41.0  
Expected frequency of 2: 12.5  

####12c.Compare the observed and expected frequencies in a graph. Describe how they differ.

The observed frequency of bears with 0 or 2 b alleles is greater than our expected frequency while the observed frequency of bears with 1 b allele is smaller than our expected frequency.

```{r}

Bear <- read_csv("../Data/SpiritBearGenetics.csv")
#Calculate the fraction of b alleles in the population 
#Make a Table and Order the Categories

#Account for each bear having 2 genes and some having Bb
b_fraction <- (21*2+24)/(87*2)
b_fraction

#Next calculate expected frequency of bears with 0, 1, and 2 copies: bb = 2, Bb = 1, BB = 0

my_frequency <- dbinom(0:2, size = 2, prob = b_fraction)
expected <- 87*my_frequency
observed <- c(42,24,21)
#Compare the observed and expected frequencies in a graph
table = rbind(observed, expected)
colnames(table) = c("BB", "Bb", "bb")
barplot(table, beside = TRUE, legend.text = c("observed", "expected"))
```

##Chapter 8
#### 24a. Graph the relative frequency distribution for these results. What type of graph is ideal?

A discrete graph like a bar graph is ideal because this is not continuous data. 

```{r}
Dodder <- read_csv("../Data/DodderGrowth.csv")
Dodder <- Dodder %>%
  group_by(directionOfGrowth) %>%
  tally() %>%
  mutate(relative_frequency = n/30)

Dodder_Graph <- ggplot(data = Dodder, mapping = aes(x = directionOfGrowth, y = relative_frequency, fill = directionOfGrowth)) +
  geom_col()+
  labs(x = "Direction of Growth", y = "Relative Frequency")
```

####24b. What are the relative frequencies expected if the parasite is unable to detect the plant volatiles or any other cues present? Add these expected relative frequencies to your graph in part (a)

The red line indicates expected relative frequencies. This is essentially the null Hypothesis. Since there are 4 groups it would be expected that each would account for 1/4 of observations (0.25)

```{r}
#Add a column to the data that has expected frequency
Expected <- Dodder %>%
  mutate(expected_frequency = (30/4))

#Each should be 0.25, so plot everything together
Dodder_Graph +
  geom_hline(yintercept = 0.25, color = "darkred")
```

####24c. Using these data, calculate the fraction of seedlings that grow toward the volatiles. What does this fraction estimate?

```{r}
probability <- 17/30
```

The fraction of seedlings that grow toward the volatiles is 17/30 = 0.57. This fraction tells us, within this sample pool, how likely the individuals were to grow towards a volatile. 

####24d. Provide a standard error for your estimate. What does this standard error represent?

Standard Error = 0.090 representing a relatively small standard error. This means we are strongly confident in our estimate of probability. 

```{r}
#The standard error of proportions is equal to:
# sqrt[p(1-p)/n]

Dodder_SE <- sqrt((0.57*(1-0.57))/30)

```

####24e. Calculate the range of most-plausible values for the fraction of dodder seedlings that grow toward the volatiles under these experimental conditions. Does it include or exclude the fraction expected if the parasite is unable to detect plant volatiles or other cues present?

The range of most-plausible values for the fraction of dodder seedlings that grow toward the volatiles under these experimental condictions is between 0.38<mean<0.75. This range includes the fractions expected if the parasite is unable to detect plant volatiles or other cues present. 
```{r}
Upper_Conf <- probability - (2*Dodder_SE)
Lower_Conf <- probability + (2*Dodder_SE)
Upper_Conf
Lower_Conf
```

##Chapter 9
####16a.Calculate expected frequencies for a contingency test.

```{r}
Prairie <- read_csv("../Data/PrairieDogMating.csv")

#Expected frequency is: 
#row total times a column total divided by the grand total

Prairie_Table <- table(Prairie)
One_B <- (87*249)/263
Two_B <- (93*249)/263
Three_B <- (61*249)/263
Four_B <- (17*249)/263
Five_B <- (5*249)/263
One_No_B <- (87*14)/263
Two_No_B <-(93*14)/263
Three_No_B <-(61*14)/263
Four_No_B <- (17*14)/263
Five_No_B <- (5*14)/263


One_B
Two_B
Three_B
Four_B
Five_B
One_No_B
Two_No_B
Three_No_B
Four_No_B
Five_No_B
```

####16b. Examine the expected frequencies. Do they meet the assumptions of a ChiSq contingency test? If not, what steps could you take to meet the assumptions and make a test?

No, our data does not meet the assumptions of a ChiSq contingency test. Many of our frequencies are less than one. We could meet the assumptions by adding 1 to all of our data points, combining groups to get higher values, or doing a permutation test (or other statistic test) instead.

####16c. An appropriate test shows that the number of mates of the female prairie dogs is associated with giving birth. Does this mean that the mating with more males increases the probability of giving birth? Can you think of an alternative explanation?

Correlation does not necessarily mean causation. There may be some other factor acting on both. For example, maybe the hormones associated with multiple mates influences birth but this was not tested. 

##Question 27

The p value is below the significance level of alpha = 0.05. Therefore, we reject the null hypothesis. The probability of deterioration in health is influenced by whether a wife is widowed or not. **This question is sad and I don't like it.**

```{r}
Widows <- read.csv("../Data/WidowHealth.csv")

summary(Widows)

WidowTable = table(Widows$health_deterioration, Widows$widowed)
addmargins(WidowTable)

chisq.test(Widows$health_deterioration, Widows$widowed, correct = FALSE)
```

#2. W&S T-Test Questions

##Chapter 11 

####21a. Draw a graph of the data, following recommended principles of good graph design. What trend is suggested? 

Our graph suggests a general trend of a decrease in soil lead after the hurricane. 

```{r}
Soil <- read_csv("../Data/SoilLead.csv")
ggplot(data = Soil, mapping = aes(x = Site, 
        y = Change)) +
  labs(y = "Change in Soil Lead Concentration")+
geom_col()
```

####21b. Determine the most-plausible range of values for the mean change in soil lead. Describe in words what the nature of that change is. Is an increase in soil lead consistent with the data? Is a decrease in soil lead consistent?

The most plausible range of change in soil lead is between -201.46 and -48.31. A decrease in soil lead is most consistent with the data.  

```{r}
tidy(t.test(Soil$Change))
```


####21c. Test whether mean soil lead changed after the hurricanes. 

```{r}
meanbefore <- mean(Soil$Soil_lead_Before_Katrina)
meanafter <- mean(Soil$Soil_lead_After_Katrina)
meanbefore
meanafter
```
Mean soil lead changed (decreased) from 331.75 to 206.84 after the hurricanes.


##Chapter 12 
####20a. What is the difference between areas upstream and downstream? What is the 95% Confidence Interval of the mean difference?

The mean difference of the difference between upstream and downstream areas is 1.84 with a 95% Confidence Interval of (-3.9, 0.28)
```{r}
species <- read_csv("../Data/ElectricFish.csv")
species <- species %>%
  mutate(difference = speciesUpstream-speciesDownstream)
#test the distribution
qqnorm(species$difference)
#doesn't look terrible 
#do the t-test

t.test(species$difference,
       unequal.var = TRUE)
```
####20b. Test the hypothesis that the tributaries have no effect on number of species.

Our null hypothesis would be that the mean differences are equal to zero with an alternative hypothesis that they are not equal to zero. From the output of our t-test we see that our tests statistic is 1.91 and our p-value is 0.0826. This is a weak p-value and if we use a 0.05 alpha we fail to reject the null hypothesis. 

####20c. State the assumptions that you had to make to complete parts (a) and (b)

Assumption 1: The tributaries in the sample are a simple random sample of all tributaries.   
Assumption 2: The distribution of the difference between the number of species downstream and the number of species upstream is approximately symmetric.

####Question 26. Do dominant and subordinate individuals differ in the means of giggle spectral CV?

We reject the null hypothesis because p is low: 0.02. We are able to conclude that subordinate individuals differ in means of giggle spectral CV.

```{r}
Giggles <- read_csv("../Data/HyenaGiggles.csv")
#t.test to see if groups differ
head(Giggles)
library(broom)
Test <- t.test(Giggles$dominantIndividualGiggleVariation, y = Giggles$subordinateIndividualGiggleVariation,
               data = Giggles,
               paired = TRUE)

Test
```

####Question 30. 
Using a two-sample t-test looks at the difference between groups. This is not what the researchers meant to look for - they were not attempting to compare the groups but rather compare if each group was able to detect cues. The groups should be tested seperately with a one-tailed t-test to determine if each individual group is able to detect the cues.  

#3 Power and T
####3.1 Data Generating Process
Write a function that takes two means, two standard deviations, and two sample sizes as arguments. Have it return a data frame or tibble based on the inputs ready to go for a t-test

```{r}
make_t_data <- function(mean1, mean2, s1, s2, n1, n2, equal.variance = FALSE) {
  #make a data frame, repeating treatments n number of times
  #Use rnorm for each of the values
data.frame(treatment = c(rep("A", n1), rep("B", n2)),
             value = rnorm(n1+n2,
             mean = c(rep(mean1, n1), rep(mean2, n2)), sd = c(rep(s1, n1), rep(s2, n2))))
}
#test if it works by applying the function to return a data frame based on the inputs.

my_data <- make_t_data(mean1 = 5, mean2 = 8, s1 = 1, s2 = 2, n1 = 10, n2 = 15)
```

###3.2 P from T
Write a function that takes a data frame and runs a two-tailed t-test with the variances assumed to be unequal. Show it works by comparing its p-value to that returned by t-test for the same simulated data set. +2 Extra Credit, look at ?ifelse or ?"if" and use one of them to have your function chose to use unequal variances if your variances differ by 20%


```{r}
get_p <- function(sim_data) {
  test <- t.test(value ~ treatment, 
                 data = sim_data)
  test$p.value
  
}

get_p(make_t_data(mean1 = 5, mean2 = 8, s1 = 1, s2 = 2, 
                  n1 = 10, n2 =  15))
```

####3.3 So Many Ps!
Write a function that takes some number of simulations, two means, two standard deviations, and two samples sizes as arguments and returns a vector of p values equal in length to that number of simulations. 

```{r}
replicate_function <- function(mean1, mean2, s1, s2, n1, n2) {
  p <- replicate(20,
          get_p(make_t_data(mean1, mean2, s1, s2, n1, n2)))
}

p_data <- replicate_function(mean1 = 5, mean2 = 8, 
                  s1 = 1,s2 = 2, n1 = 10, n2 = 15)
```

####3.4 Power
Write a functions that takes an alpha value, some number of simulations, two means, two standard deviations, and two sample sizes as arguments, and returns the power. 

```{r}
get_power <- function(mean1, mean2, s1, s2, n1, n2, nsims = 100, alpha = 0.07) {
  p <- replicate(nsims, 
                 get_p(make_t_data(mean1, mean2, s1, s2, n1, n2)))
  num_wrong <- sum(p>alpha)
  1 - num_wrong/nsims
}

get_power(1, 2, 1, 2, 3, 5, 100, 0.07)
```

####3.5 Show it works
Using your functions from above, explore how changing the difference between the means of two groups interacts with the difference between two standard deviations of groups to affect the power of a t-test. Explain the results you produce.

As difference in mean increases this could be compared to a large effect size - larger differences infer high power in detecting those differences. When standard deviation is added to this - as variance decreases the power increases (there is less noise). This creates results where high mean difference and low standard deviation have high power.

```{r}
pow_frame <- 
  crossing(diff_m = 1:4, diff_s = 1:4, n1 = 10, n2 = 15) %>%
  rowwise() %>%
  mutate(power = get_power(mean1 = 0, mean2 = diff_m, s1 = 0, s2 = diff_s, n1 = n1, n2 = n2, nsims = 100, alpha = 0.05)) %>%
  ungroup()

ggplot(data = pow_frame,
       mapping = aes(x = diff_m, 
                     y = power, 
                     color = factor(diff_s)))+
  geom_point()+
  geom_line()+
  scale_color_manual(values = beyonce_palette(18, n=4, type = "continuous")) +
  labs(x = "Mean Difference", y = "Power", color = "SD")
```

###3.6 Extra Credit
+2 Extra credit if you include a comparison between running the test with versus without equal variances - this might require you to re-write your function from 3.2 to include an argument where you specify if you want equal or unequal variance tests to be used. 

```{r}
get_p <- function(sim_data) {
  test <- t.test(value ~ treatment, 
                 data = sim_data, var.equal = TRUE)
  test$p.value
  
}

get_p(make_t_data(mean1 = 5, mean2 = 8, s1 = 1, s2 = 2, 
                  n1 = 10, n2 =  15))
```
